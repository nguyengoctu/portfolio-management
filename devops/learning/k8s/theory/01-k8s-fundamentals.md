# Kubernetes Fundamentals - Giáº£i thÃ­ch chi tiáº¿t

## 1. Kubernetes lÃ  gÃ¬ vÃ  táº¡i sao cáº§n nÃ³?

### Váº¥n Ä‘á» trÆ°á»›c khi cÃ³ Kubernetes

TÆ°á»Ÿng tÆ°á»£ng báº¡n cÃ³ má»™t website bÃ¡n hÃ ng vá»›i:
- **Frontend**: React app
- **Backend**: API viáº¿t báº±ng Spring Boot  
- **Database**: MySQL
- **Cache**: Redis

**Vá»›i Docker thÃ´ng thÆ°á»ng:**
```bash
# Báº¡n pháº£i cháº¡y tá»«ng container thá»§ cÃ´ng
docker run -d --name mysql mysql:8.0
docker run -d --name redis redis:6.0
docker run -d --name backend --link mysql --link redis my-api:1.0
docker run -d --name frontend --link backend my-frontend:1.0
```

**Váº¥n Ä‘á» phÃ¡t sinh:**
1. **Náº¿u container backend crash** â†’ Ai restart nÃ³?
2. **Traffic tÄƒng cao** â†’ LÃ m sao cháº¡y thÃªm 5 containers backend?  
3. **Server die** â†’ Containers máº¥t háº¿t, pháº£i setup láº¡i tá»« Ä‘áº§u
4. **Deploy version má»›i** â†’ Pháº£i stop toÃ n bá»™ rá»“i start láº¡i (downtime)
5. **100 containers** â†’ Quáº£n lÃ½ thá»§ cÃ´ng = Ä‘á»‹a ngá»¥c

### Kubernetes giáº£i quyáº¿t nhÆ° tháº¿ nÃ o?

Kubernetes nhÆ° má»™t "ngÆ°á»i quáº£n lÃ½ thÃ´ng minh" tá»± Ä‘á»™ng:
- **Self-healing**: Container crash? Tá»± Ä‘á»™ng táº¡o cÃ¡i má»›i
- **Auto-scaling**: Traffic cao? Tá»± Ä‘á»™ng thÃªm containers  
- **Zero-downtime deployment**: Deploy khÃ´ng cáº§n táº¯t service
- **Load balancing**: Tá»± Ä‘á»™ng phÃ¢n tÃ¡n traffic
- **Service discovery**: Containers tá»± tÃ¬m tháº¥y nhau

**VÃ­ dá»¥ thá»±c táº¿:**
```yaml
# Báº¡n chá»‰ cáº§n nÃ³i: "TÃ´i muá»‘n 3 containers backend luÃ´n cháº¡y"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
spec:
  replicas: 3  # LuÃ´n cÃ³ 3 containers
  template:
    spec:
      containers:
      - name: api
        image: my-api:1.0
```

Kubernetes sáº½:
- Táº¡o 3 containers backend
- Náº¿u 1 container cháº¿t â†’ Tá»± táº¡o cÃ¡i má»›i
- Náº¿u server cháº¿t â†’ Chuyá»ƒn containers sang server khÃ¡c
- Náº¿u muá»‘n update â†’ Rolling update tá»«ng cÃ¡i má»™t (khÃ´ng downtime)

## 2. Kiáº¿n trÃºc Kubernetes - Giáº£i thÃ­ch báº±ng vÃ­ dá»¥ thá»±c táº¿

### HÃ£y tÆ°á»Ÿng tÆ°á»£ng Kubernetes nhÆ° má»™t cÃ´ng ty

```
ğŸ¢ CÃ”NG TY KUBERNETES
â”œâ”€â”€ ğŸ‘” BAN GIÃM Äá»C (Control Plane/Master Node)
â”‚   â”œâ”€â”€ ğŸ¯ CEO (API Server) - Nháº­n má»i yÃªu cáº§u, ra quyáº¿t Ä‘á»‹nh
â”‚   â”œâ”€â”€ ğŸ“‹ ThÆ° kÃ½ (etcd) - Ghi nhá»› má»i thÃ´ng tin cÃ´ng ty
â”‚   â”œâ”€â”€ ğŸ‘¥ HR (Scheduler) - Quyáº¿t Ä‘á»‹nh nhÃ¢n viÃªn lÃ m á»Ÿ phÃ²ng nÃ o
â”‚   â””â”€â”€ ğŸ“Š Manager (Controller) - Äáº£m báº£o má»i thá»© cháº¡y Ä‘Ãºng káº¿ hoáº¡ch
â”‚
â””â”€â”€ ğŸ­ CÃC PHÃ’NG BAN (Worker Nodes)
    â”œâ”€â”€ ğŸ‘¨â€ğŸ’¼ TrÆ°á»Ÿng phÃ²ng (kubelet) - Quáº£n lÃ½ nhÃ¢n viÃªn trong phÃ²ng
    â”œâ”€â”€ ğŸ“ Tá»•ng Ä‘Ã i (kube-proxy) - Káº¿t ná»‘i cÃ¡c phÃ²ng vá»›i nhau
    â””â”€â”€ ğŸ‘·â€â™‚ï¸ NhÃ¢n viÃªn (Containers) - LÃ m viá»‡c thá»±c táº¿
```

### Master Node (Control Plane) - Ban GiÃ¡m Äá»‘c

#### ğŸ¯ API Server (CEO)
```bash
# Khi báº¡n cháº¡y lá»‡nh:
kubectl create deployment nginx --image=nginx --replicas=3

# API Server nháº­n lá»‡nh vÃ  nÃ³i:
# "OK, tÃ´i cáº§n táº¡o 3 containers nginx. Äá»ƒ tÃ´i kiá»ƒm tra vÃ  thá»±c hiá»‡n..."
```
- Nháº­n táº¥t cáº£ requests tá»« kubectl, web UI, hoáº·c cÃ¡c tools khÃ¡c
- XÃ¡c thá»±c, phÃ¢n quyá»n (authentication & authorization)
- Giao tiáº¿p vá»›i táº¥t cáº£ components khÃ¡c

#### ğŸ“‹ etcd (ThÆ° kÃ½)
```bash
# etcd lÆ°u trá»¯ má»i thá»©:
{
  "deployments": {
    "nginx": {
      "replicas": 3,
      "image": "nginx",
      "status": "running"
    }
  },
  "nodes": ["worker1", "worker2", "worker3"],
  "pods": [...]
}
```
- Database phÃ¢n tÃ¡n, lÆ°u trá»¯ toÃ n bá»™ tráº¡ng thÃ¡i cluster
- Má»i thÃ´ng tin vá» pods, services, configs Ä‘á»u á»Ÿ Ä‘Ã¢y
- **Náº¿u etcd máº¥t â†’ Cluster cháº¿t hoÃ n toÃ n**

#### ğŸ‘¥ Scheduler (HR Department)
```bash
# Scheduler nghÄ©:
# "Cáº§n táº¡o 3 pods nginx. Äá»ƒ xem..."
# "Worker1: CPU 50%, RAM 60% - OK"
# "Worker2: CPU 90%, RAM 80% - QuÃ¡ táº£i!"
# "Worker3: CPU 30%, RAM 40% - Tá»‘t nháº¥t!"
# "Quyáº¿t Ä‘á»‹nh: Pod1 â†’ Worker1, Pod2 â†’ Worker3, Pod3 â†’ Worker1"
```
- Quyáº¿t Ä‘á»‹nh pod cháº¡y trÃªn node nÃ o dá»±a trÃªn:
  - Resource availability (CPU, RAM)
  - Node constraints (labels, taints)
  - Pod requirements (resource requests)

#### ğŸ“Š Controller Manager (CÃ¡c Manager)
```bash
# Deployment Controller kiá»ƒm tra:
# "Spec nÃ³i cáº§n 3 replicas, hiá»‡n táº¡i cÃ³ 2. Thiáº¿u 1!"
# "Táº¡o thÃªm 1 pod ná»¯a..."

# ReplicaSet Controller:
# "Pod nginx-123 cháº¿t rá»“i! Táº¡o pod má»›i thay tháº¿!"

# Node Controller:
# "Worker2 khÃ´ng pháº£n há»“i 5 phÃºt rá»“i. Chuyá»ƒn pods sang node khÃ¡c!"
```
- **Deployment Controller**: Quáº£n lÃ½ deployments
- **ReplicaSet Controller**: Äáº£m báº£o sá»‘ lÆ°á»£ng pods
- **Node Controller**: Theo dÃµi tráº¡ng thÃ¡i nodes
- **Endpoint Controller**: Cáº­p nháº­t service endpoints

### Worker Nodes - CÃ¡c PhÃ²ng Ban

#### ğŸ‘¨â€ğŸ’¼ kubelet (TrÆ°á»Ÿng phÃ²ng)
```bash
# kubelet nháº­n lá»‡nh tá»« API Server:
# "Anh cáº§n cháº¡y pod nginx vá»›i image nginx:1.20"

# kubelet thá»±c hiá»‡n:
1. Pull image nginx:1.20
2. Táº¡o container
3. Monitor container health
4. BÃ¡o cÃ¡o tráº¡ng thÃ¡i vá» API Server
```
- Agent cháº¡y trÃªn má»—i worker node
- Nháº­n pod specs tá»« API Server
- Quáº£n lÃ½ container lifecycle (create, start, stop, restart)
- Health monitoring vÃ  reporting

#### ğŸ“ kube-proxy (Tá»•ng Ä‘Ã i viÃªn)
```bash
# Khi cÃ³ service:
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
  - port: 80

# kube-proxy táº¡o iptables rules:
# "Traffic Ä‘áº¿n nginx-service:80 â†’ Load balance giá»¯a 3 pods nginx"
```
- Network proxy cháº¡y trÃªn má»—i node
- Implement Kubernetes Services
- Load balancing traffic giá»¯a pods
- Maintain network rules (iptables/ipvs)

#### ğŸ³ Container Runtime
```bash
# kubelet nÃ³i vá»›i container runtime:
# "Táº¡o container vá»›i image nginx:1.20"

# Container runtime (Docker/containerd):
# "OK, Ä‘ang pull image... Táº¡o container... Start container... Done!"
```
- Docker, containerd, CRI-O
- Thá»±c táº¿ táº¡o vÃ  cháº¡y containers
- Managed bá»Ÿi kubelet

### Workflow thá»±c táº¿

**VÃ­ dá»¥: Táº¡o deployment nginx vá»›i 3 replicas**

```bash
kubectl create deployment nginx --image=nginx --replicas=3
```

**QuÃ¡ trÃ¬nh diá»…n ra:**

1. **kubectl** â†’ **API Server**: "Táº¡o deployment nginx vá»›i 3 replicas"

2. **API Server**:
   - Validate request
   - LÆ°u vÃ o **etcd**: "Deployment nginx cáº§n 3 pods"
   - Notify **Controller Manager**

3. **Deployment Controller** (part of Controller Manager):
   - Äá»c etcd: "Cáº§n 3 pods nhÆ°ng hiá»‡n cÃ³ 0"
   - Táº¡o **ReplicaSet** vá»›i 3 pod specs
   - LÆ°u ReplicaSet vÃ o etcd

4. **Scheduler**:
   - Äá»c etcd: "3 pods chÆ°a Ä‘Æ°á»£c assign node"
   - Evaluate nodes: "Worker1 tá»‘t nháº¥t cho pod1, Worker2 cho pod2..."
   - Update etcd: "Pod1 â†’ Worker1, Pod2 â†’ Worker2, Pod3 â†’ Worker1"

5. **kubelet** trÃªn má»—i worker:
   - Watch etcd changes
   - Worker1 kubelet: "TÃ´i cáº§n cháº¡y pod1 vÃ  pod3"
   - Pull nginx image
   - Táº¡o containers
   - Report tráº¡ng thÃ¡i vá» API Server

6. **kube-proxy**:
   - Náº¿u cÃ³ Service â†’ Setup network rules
   - Load balance traffic giá»¯a 3 pods

**Káº¿t quáº£ cuá»‘i cÃ¹ng:**
```bash
kubectl get pods
# NAME                     READY   STATUS    RESTARTS   AGE
# nginx-7d8b49557f-abc123  1/1     Running   0          30s
# nginx-7d8b49557f-def456  1/1     Running   0          30s  
# nginx-7d8b49557f-ghi789  1/1     Running   0          30s
```

### Táº¡i sao thiáº¿t káº¿ nhÆ° váº­y?

1. **Separation of Concerns**: Má»—i component cÃ³ trÃ¡ch nhiá»‡m riÃªng
2. **Scalability**: CÃ³ thá»ƒ cÃ³ nhiá»u worker nodes
3. **High Availability**: Master components cÃ³ thá»ƒ cháº¡y nhiá»u instances
4. **Modularity**: CÃ³ thá»ƒ thay tháº¿ tá»«ng component (VD: Docker â†’ containerd)

---
## ğŸ§ª **HANDS-ON EXERCISE**
**[Exercise 01: Architecture Exploration](../exercises/ex01-architecture-exploration.md)**

KhÃ¡m phÃ¡ kiáº¿n trÃºc cluster thá»±c táº¿ vÃ  plan architecture cho portfolio project.
- Identify control plane components
- Understand node architecture  
- Plan portfolio services mapping
- Practice troubleshooting

---

## 3. Core Objects - Hiá»ƒu báº±ng vÃ­ dá»¥ thá»±c táº¿

### ğŸ  Pod - CÄƒn há»™ chá»©a containers

**Pod giá»‘ng nhÆ° má»™t cÄƒn há»™:**
- 1 cÄƒn há»™ cÃ³ thá»ƒ cÃ³ 1 hoáº·c nhiá»u ngÆ°á»i á»Ÿ (containers)
- CÃ¹ng chia sáº» Ä‘iá»‡n nÆ°á»›c internet (network & storage)
- Náº¿u cÄƒn há»™ bá»‹ phÃ¡ â†’ Má»i ngÆ°á»i trong Ä‘Ã³ Ä‘á»u máº¥t

```yaml
# VÃ­ dá»¥: CÄƒn há»™ cÃ³ 1 ngÆ°á»i (container nginx)
apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  containers:
  - name: nginx
    image: nginx:1.20
    ports:
    - containerPort: 80
```

**Thá»±c táº¿:**
```bash
# Táº¡o pod
kubectl run web-server --image=nginx

# Kiá»ƒm tra
kubectl get pods
# NAME         READY   STATUS    RESTARTS   AGE
# web-server   1/1     Running   0          10s

# VÃ o trong pod
kubectl exec -it web-server -- bash
```

**Multi-container Pod:**
```yaml
# VÃ­ dá»¥: CÄƒn há»™ cÃ³ 2 ngÆ°á»i - nginx vÃ  logger
apiVersion: v1
kind: Pod
metadata:
  name: web-with-logger
spec:
  containers:
  - name: nginx
    image: nginx:1.20
    volumeMounts:
    - name: logs
      mountPath: /var/log/nginx
  - name: logger
    image: busybox
    command: ["tail", "-f", "/logs/access.log"]
    volumeMounts:
    - name: logs
      mountPath: /logs
  volumes:
  - name: logs
    emptyDir: {}
```

**Váº¥n Ä‘á» vá»›i Pod:**
- Pod cháº¿t â†’ Máº¥t luÃ´n
- KhÃ´ng tá»± Ä‘á»™ng restart náº¿u crash
- Pháº£i táº¡o thá»§ cÃ´ng tá»«ng cÃ¡i

### ğŸ¢ ReplicaSet - CÃ´ng ty xÃ¢y nhiá»u cÄƒn há»™ giá»‘ng nhau

**ReplicaSet Ä‘áº£m báº£o luÃ´n cÃ³ N pods giá»‘ng nhau:**

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-replicaset
spec:
  replicas: 3  # LuÃ´n cÃ³ 3 pods
  selector:
    matchLabels:
      app: nginx
  template:    # Template Ä‘á»ƒ táº¡o pod
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
```

**Thá»±c táº¿:**
```bash
kubectl apply -f replicaset.yaml

# Check
kubectl get replicasets
kubectl get pods
# Sáº½ cÃ³ 3 pods nginx

# Test self-healing: XÃ³a 1 pod
kubectl delete pod <pod-name>
# ReplicaSet sáº½ tá»± táº¡o pod má»›i thay tháº¿
```

**NhÆ°ng ReplicaSet cÃ³ váº¥n Ä‘á»:**
- KhÃ³ update image version
- KhÃ´ng cÃ³ rolling update
- Pháº£i quáº£n lÃ½ thá»§ cÃ´ng

### ğŸ—ï¸ Deployment - CÃ´ng ty xÃ¢y dá»±ng thÃ´ng minh

**Deployment = ReplicaSet + Rolling Updates + Rollback**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.20  # Version cÅ©
```

**Ká»‹ch báº£n thá»±c táº¿ - Rolling Update:**
```bash
# 1. Deploy version 1.20
kubectl apply -f deployment.yaml

# 2. Check pods
kubectl get pods
# 3 pods vá»›i nginx:1.20

# 3. Update lÃªn version 1.21
kubectl set image deployment/nginx-deployment nginx=nginx:1.21

# 4. Watch rolling update
kubectl rollout status deployment/nginx-deployment

# QuÃ¡ trÃ¬nh diá»…n ra:
# - Táº¡o 1 pod má»›i vá»›i nginx:1.21
# - Khi pod má»›i ready â†’ XÃ³a 1 pod cÅ©
# - Láº·p láº¡i cho Ä‘áº¿n khi cÃ³ 3 pods má»›i
# - Zero downtime!
```

**Rollback náº¿u cÃ³ lá»—i:**
```bash
# Version má»›i cÃ³ bug, rollback ngay
kubectl rollout undo deployment/nginx-deployment

# Hoáº·c rollback to version cá»¥ thá»ƒ
kubectl rollout undo deployment/nginx-deployment --to-revision=1
```

### ğŸŒ Service - Há»‡ thá»‘ng Ä‘á»‹nh tuyáº¿n thÃ´ng minh

**Váº¥n Ä‘á»:** Pod cÃ³ IP thay Ä‘á»•i liÃªn tá»¥c, lÃ m sao client káº¿t ná»‘i?

**Service = Load Balancer + DNS name cá»‘ Ä‘á»‹nh**

#### ClusterIP Service (Internal)
```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx  # Chá»n pods cÃ³ label app=nginx
  ports:
  - port: 80      # Port cá»§a service
    targetPort: 80 # Port cá»§a pods
  type: ClusterIP  # Chá»‰ access tá»« trong cluster
```

**Thá»±c táº¿:**
```bash
kubectl apply -f service.yaml

# Service táº¡o DNS name: nginx-service
# Pods khÃ¡c cÃ³ thá»ƒ access: http://nginx-service

# Test tá»« pod khÃ¡c
kubectl run test --image=busybox --rm -it -- sh
/ # wget -qO- http://nginx-service
# Sáº½ load balance giá»¯a 3 pods nginx
```

#### NodePort Service (External)
```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-nodeport
spec:
  selector:
    app: nginx
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080  # Port trÃªn node
  type: NodePort
```

**Access tá»« bÃªn ngoÃ i:**
```bash
# Láº¥y IP cá»§a node
kubectl get nodes -o wide

# Access tá»« browser hoáº·c curl
curl http://<node-ip>:30080
```

#### LoadBalancer Service (Cloud)
```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-lb
spec:
  selector:
    app: nginx
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer  # Cloud provider táº¡o LB
```

### ğŸ—„ï¸ ConfigMap - File cáº¥u hÃ¬nh

**ConfigMap chá»©a configuration khÃ´ng nháº¡y cáº£m:**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  database_url: "mysql://localhost:3306/mydb"
  debug_mode: "true"
  max_connections: "100"
```

**Sá»­ dá»¥ng trong Pod:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app
    image: my-app:1.0
    env:
    - name: DATABASE_URL
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: database_url
    - name: DEBUG_MODE
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: debug_mode
```

### ğŸ” Secret - Kho máº­t kháº©u

**Secret chá»©a thÃ´ng tin nháº¡y cáº£m (mÃ£ hÃ³a base64):**

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-secrets
type: Opaque
data:
  username: YWRtaW4=     # admin (base64)
  password: cGFzc3dvcmQ= # password (base64)
```

**Táº¡o Secret tá»« command line:**
```bash
kubectl create secret generic app-secrets \
  --from-literal=username=admin \
  --from-literal=password=password
```

**Sá»­ dá»¥ng trong Pod:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app
    image: my-app:1.0
    env:
    - name: DB_USERNAME
      valueFrom:
        secretKeyRef:
          name: app-secrets
          key: username
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: app-secrets
          key: password
```

### ğŸ¢ Namespace - PhÃ¢n chia khÃ´ng gian

**Namespace = Virtual clusters trong cluster**

```bash
# Táº¡o namespace cho mÃ´i trÆ°á»ng khÃ¡c nhau
kubectl create namespace development
kubectl create namespace staging  
kubectl create namespace production

# Deploy app vÃ o namespace cá»¥ thá»ƒ
kubectl apply -f deployment.yaml -n development
kubectl apply -f deployment.yaml -n production

# List resources theo namespace
kubectl get pods -n development
kubectl get pods -n production

# Set default namespace
kubectl config set-context --current --namespace=development
```

**Namespace isolation:**
```yaml
# development/app-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  namespace: development
spec:
  replicas: 1  # Dev chá»‰ cáº§n 1 pod
  # ...

---
# production/app-deployment.yaml  
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  namespace: production
spec:
  replicas: 10  # Production cáº§n 10 pods
  # ...
```

### ğŸ”„ Tá»•ng há»£p workflow thá»±c táº¿

**Scenario: Deploy má»™t web application**

1. **Táº¡o namespace:**
```bash
kubectl create namespace my-app
```

2. **Táº¡o ConfigMap:**
```bash
kubectl create configmap app-config \
  --from-literal=API_URL=https://api.example.com \
  -n my-app
```

3. **Táº¡o Secret:**
```bash
kubectl create secret generic app-secrets \
  --from-literal=API_KEY=secret123 \
  -n my-app
```

4. **Deploy application:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: my-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: web
        image: my-web-app:v1.0
        env:
        - name: API_URL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: API_URL
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: API_KEY
```

5. **Expose vá»›i Service:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-app-service
  namespace: my-app
spec:
  selector:
    app: web-app
  ports:
  - port: 80
    targetPort: 3000
  type: LoadBalancer
```

6. **Deploy vÃ  test:**
```bash
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml

# Check status
kubectl get all -n my-app

# Test application
kubectl port-forward service/web-app-service 8080:80 -n my-app
curl http://localhost:8080
```

**Káº¿t quáº£:**
- 3 pods cháº¡y web application  
- Load balancer phÃ¢n tÃ¡n traffic
- Config vÃ  secrets Ä‘Æ°á»£c inject vÃ o containers
- Zero downtime updates vá»›i rolling deployment

---
## ğŸ§ª **HANDS-ON EXERCISES**

**[Exercise 02: Pod Fundamentals](../exercises/ex02-pod-fundamentals.md)**

Thá»±c hÃ nh táº¡o pods cho portfolio services vá»›i ConfigMaps vÃ  Secrets.
- Single vÃ  multi-container pods
- ConfigMap vÃ  Secret integration
- Health checks implementation
- Inter-pod communication testing

**[Exercise 03: Services & Networking](../exercises/ex03-services-networking.md)**

Setup networking vÃ  service discovery cho portfolio services.
- ClusterIP vs NodePort vs LoadBalancer
- Service discovery vÃ  DNS
- Load balancing testing
- Troubleshooting connectivity

**[Exercise 04: Deployments & ReplicaSets](../exercises/ex04-deployments-replicasets.md)**

Convert pods thÃ nh production-ready deployments.
- Scaling vÃ  self-healing
- Rolling updates vÃ  rollbacks  
- Deployment strategies
- Complete portfolio deployment

---

## 4. Health Checks - Äáº£m báº£o á»©ng dá»¥ng khá»e máº¡nh

### ğŸ¥ Probes - BÃ¡c sÄ© kiá»ƒm tra sá»©c khá»e

Kubernetes cÃ³ 3 loáº¡i "bÃ¡c sÄ©" kiá»ƒm tra pods:

#### Liveness Probe - "CÃ²n sá»‘ng khÃ´ng?"
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-app
spec:
  containers:
  - name: app
    image: my-app:1.0
    livenessProbe:
      httpGet:
        path: /health      # Endpoint kiá»ƒm tra health
        port: 8080
      initialDelaySeconds: 30  # Äá»£i 30s sau khi start
      periodSeconds: 10        # Kiá»ƒm tra má»—i 10s
```

**Thá»±c táº¿:**
- Náº¿u `/health` tráº£ vá» lá»—i â†’ K8s restart container
- TrÃ¡nh trÆ°á»ng há»£p app "zombie" (cháº¡y nhÆ°ng khÃ´ng hoáº¡t Ä‘á»™ng)

#### Readiness Probe - "Sáºµn sÃ ng phá»¥c vá»¥ chÆ°a?"
```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
```

**Thá»±c táº¿:**
- Náº¿u `/ready` tráº£ vá» lá»—i â†’ Service khÃ´ng route traffic Ä‘áº¿n pod nÃ y
- Pod váº«n cháº¡y, chá»‰ khÃ´ng nháº­n requests

#### Startup Probe - "ÄÃ£ start xong chÆ°a?"
```yaml
startupProbe:
  httpGet:
    path: /startup
    port: 8080
  periodSeconds: 5
  failureThreshold: 30  # Tá»‘i Ä‘a 30 * 5s = 150s Ä‘á»ƒ start
```

**Khi nÃ o dÃ¹ng:**
- App startup cháº­m (Java applications, database connections)
- TrÃ¡nh liveness probe kill app Ä‘ang starting

---
## ğŸ§ª **HANDS-ON EXERCISE**
**[Exercise 05: Health Checks & Resource Management](../exercises/ex05-health-resources.md)**

Implement production-ready health checks vÃ  resource management.
- Liveness, readiness, vÃ  startup probes
- Resource requests/limits optimization
- Quality of Service classes
- Horizontal Pod Autoscaler setup

---

## 5. Resource Management - Quáº£n lÃ½ tÃ i nguyÃªn

### ğŸ’° CPU vÃ  Memory - NgÃ¢n sÃ¡ch cho containers

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-demo
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:    # "TÃ´i cáº§n Ã­t nháº¥t"
        memory: "64Mi"   # 64 MB RAM
        cpu: "250m"      # 0.25 CPU core
      limits:      # "TÃ´i khÃ´ng Ä‘Æ°á»£c vÆ°á»£t quÃ¡"
        memory: "128Mi"  # 128 MB RAM  
        cpu: "500m"      # 0.5 CPU core
```

**Requests vs Limits:**
- **Requests**: Kubernetes Ä‘áº£m báº£o cÃ³ Ã­t nháº¥t resources nÃ y
- **Limits**: Container khÃ´ng Ä‘Æ°á»£c vÆ°á»£t quÃ¡

**CPU Units:**
- `1000m = 1` CPU core
- `500m = 0.5` CPU core  
- `250m = 0.25` CPU core

### ğŸ·ï¸ Quality of Service (QoS)

1. **Guaranteed** (requests = limits):
```yaml
resources:
  requests:
    memory: "128Mi"
    cpu: "500m"
  limits:
    memory: "128Mi"  # Same as requests
    cpu: "500m"      # Same as requests
```

2. **Burstable** (requests < limits):
```yaml
resources:
  requests:
    memory: "64Mi"
    cpu: "250m"
  limits:
    memory: "128Mi"  # More than requests
    cpu: "500m"
```

3. **BestEffort** (no requests/limits):
```yaml
# No resources section
```

**Æ¯u tiÃªn khi node háº¿t resources:**
Guaranteed > Burstable > BestEffort

## 6. Scaling - Tá»± Ä‘á»™ng má»Ÿ rá»™ng

### ğŸ“ˆ Horizontal Pod Autoscaler (HPA)

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Scale khi CPU > 70%
```

**Thá»±c táº¿:**
```bash
# Deploy HPA
kubectl apply -f hpa.yaml

# Táº¡o load test
kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh
# Trong pod: while true; do wget -q -O- http://web-app-service; done

# Watch scaling
kubectl get hpa -w
kubectl get pods -w
```

## 7. Storage - LÆ°u trá»¯ dá»¯ liá»‡u

### ğŸ’¾ Volume Types

#### emptyDir - ThÆ° má»¥c táº¡m
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: shared-storage
spec:
  containers:
  - name: writer
    image: busybox
    command: ["/bin/sh", "-c", "echo 'Hello' > /shared/message.txt; sleep 3600"]
    volumeMounts:
    - name: shared-data
      mountPath: /shared
  - name: reader  
    image: busybox
    command: ["/bin/sh", "-c", "cat /shared/message.txt; sleep 3600"]
    volumeMounts:
    - name: shared-data
      mountPath: /shared
  volumes:
  - name: shared-data
    emptyDir: {}  # Máº¥t khi pod bá»‹ xÃ³a
```

#### PersistentVolume - LÆ°u trá»¯ lÃ¢u dÃ i
```yaml
# PersistentVolume
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /mnt/data

---
# PersistentVolumeClaim  
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

---
# Pod sá»­ dá»¥ng PVC
apiVersion: v1
kind: Pod
metadata:
  name: storage-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: storage
      mountPath: /usr/share/nginx/html
  volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: my-pvc
```

## 8. Best Practices - Kinh nghiá»‡m thá»±c táº¿

### âœ… Resource Management
```yaml
# ALWAYS set requests vÃ  limits
resources:
  requests:
    memory: "128Mi"
    cpu: "100m"
  limits:
    memory: "256Mi"
    cpu: "200m"

# Use namespaces
metadata:
  namespace: production

# Implement health checks
livenessProbe:
  httpGet:
    path: /health
    port: 8080
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
```

### ğŸ”’ Security
```yaml
# Don't run as root
securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  
# Read-only root filesystem
securityContext:
  readOnlyRootFilesystem: true

# Drop capabilities
securityContext:
  capabilities:
    drop:
    - ALL
```

### ğŸ“Š Labels vÃ  Selectors
```yaml
# Consistent labeling
metadata:
  labels:
    app: web-app
    version: "1.0"
    environment: production
    component: frontend

# Use in selectors
spec:
  selector:
    matchLabels:
      app: web-app
      environment: production
```

## 9. Common Troubleshooting - Debug nhÆ° pro

### ğŸ› Pod khÃ´ng start
```bash
# 1. Check pod status
kubectl get pods

# 2. Describe pod  
kubectl describe pod <pod-name>

# 3. Check events
kubectl get events --sort-by=.metadata.creationTimestamp

# 4. Check logs
kubectl logs <pod-name>
kubectl logs <pod-name> --previous  # Previous container
```

### ğŸŒ Service connectivity issues
```bash
# 1. Check service endpoints
kubectl get endpoints <service-name>

# 2. Test DNS resolution
kubectl run debug --image=busybox --rm -it -- nslookup <service-name>

# 3. Test connectivity
kubectl run debug --image=busybox --rm -it -- wget -qO- http://<service-name>

# 4. Check service selector
kubectl get service <service-name> -o yaml
kubectl get pods --show-labels
```

### ğŸ” Resource issues
```bash
# Check node resources
kubectl describe nodes

# Check pod resource usage
kubectl top pods

# Check resource quotas
kubectl describe resourcequota
```

## 10. What's Next - BÆ°á»›c tiáº¿p theo

### ğŸš€ Immediate Next Steps:
1. **Setup local cluster**: minikube hoáº·c kind
2. **Practice labs**: `k8s/labs/lab01-setup-environment.md`
3. **Try kubectl commands**: `k8s/exercises/kubectl-practice.md`

### ğŸ¯ Advanced Topics:
1. **Helm** - Package manager for Kubernetes
2. **Ingress Controllers** - Advanced routing (nginx, traefik)
3. **StatefulSets** - For databases vÃ  stateful apps  
4. **Jobs & CronJobs** - Batch processing
5. **Custom Resources** - Extend Kubernetes

### ğŸ“š Real Project:
- **Portfolio Migration**: `k8s/exercises/portfolio-k8s-migration.md`
- Convert Docker Compose app â†’ Kubernetes manifests
- Learn by doing vá»›i real application

**Remember**: Kubernetes phá»©c táº¡p nhÆ°ng logical. Start vá»›i basics, practice nhiá»u, vÃ  build real applications!

---
## ğŸ§ª **FINAL PROJECT EXERCISE**
**[Portfolio K8s Migration](../exercises/portfolio-k8s-migration.md)**

Main project: Convert toÃ n bá»™ portfolio application tá»« Docker Compose sang Kubernetes.
- Complete architecture migration
- Production-ready manifests  
- StatefulSets cho database
- Ingress setup
- Monitoring vÃ  scaling

**[Kubectl Practice Commands](../exercises/kubectl-practice.md)**

Master kubectl commands vá»›i hands-on practice.
- Essential commands drilling
- Troubleshooting scenarios
- Real-world debugging
- Advanced operations

---